---
title: "Python AIå®Œå…¨å­¦ä¹ è·¯å¾„ï¼šä»é›¶åŸºç¡€åˆ°æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ"
date: 2025-11-08T11:30:00+08:00
draft: false
slug: "python-ai-learning-path"
tags: ["Python", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "æ•°æ®ç§‘å­¦"]
categories: ["æ•™ç¨‹"]
series: ["AIå­¦ä¹ è·¯å¾„"]
weight: 1
cover:
    image: "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
    alt: "AI Machine Learning"
    caption: "Python AI Learning Path"
---

# ğŸ¤– Python AIå®Œå…¨å­¦ä¹ è·¯å¾„ï¼šä»é›¶åŸºç¡€åˆ°æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ

Pythonæ˜¯äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ é¢†åŸŸæœ€å—æ¬¢è¿çš„ç¼–ç¨‹è¯­è¨€ã€‚æœ¬å­¦ä¹ è·¯å¾„å°†å¸¦ä½ ä»PythonåŸºç¡€å¼€å§‹ï¼Œé€æ­¥æˆé•¿ä¸ºä¸€ååˆæ ¼çš„AIå·¥ç¨‹å¸ˆã€‚

## ğŸ“‹ å­¦ä¹ è·¯çº¿å›¾

```
PythonåŸºç¡€ â†’ æ•°æ®åˆ†æ â†’ æœºå™¨å­¦ä¹  â†’ æ·±åº¦å­¦ä¹  â†’ ä¸“ä¸šæ–¹å‘
    â†“           â†“           â†“           â†“           â†“
  3ä¸ªæœˆ       2ä¸ªæœˆ       3ä¸ªæœˆ       3ä¸ªæœˆ       4ä¸ªæœˆ
```

## ğŸ ç¬¬ä¸€é˜¶æ®µï¼šPythonç¼–ç¨‹åŸºç¡€ (1-3ä¸ªæœˆ)

### 1.1 PythonåŸºç¡€è¯­æ³•

#### å˜é‡å’Œæ•°æ®ç±»å‹
```python
# åŸºç¡€æ•°æ®ç±»å‹
name = "å¼ ä¸‰"        # å­—ç¬¦ä¸²
age = 25            # æ•´æ•°
height = 175.5      # æµ®ç‚¹æ•°
is_student = True   # å¸ƒå°”å€¼
grades = [90, 85, 78]  # åˆ—è¡¨
profile = {"name": "æå››", "age": 30}  # å­—å…¸

# ç±»å‹è½¬æ¢
age_str = str(age)
height_int = int(height)
```

#### æ§åˆ¶æµç¨‹
```python
# æ¡ä»¶è¯­å¥
if age >= 18:
    print("æˆå¹´äºº")
elif age >= 13:
    print("é’å°‘å¹´")
else:
    print("å„¿ç«¥")

# å¾ªç¯è¯­å¥
for i in range(5):
    print(f"ç¬¬{i+1}æ¬¡å¾ªç¯")

while age < 65:
    age += 1
    print(f"å¹´é¾„å¢é•¿åˆ°{age}å²")
```

#### å‡½æ•°å®šä¹‰
```python
def calculate_bmi(weight, height):
    """è®¡ç®—BMIæŒ‡æ•°"""
    bmi = weight / (height ** 2)
    if bmi < 18.5:
        category = "åç˜¦"
    elif bmi < 24:
        category = "æ­£å¸¸"
    else:
        category = "åèƒ–"
    return round(bmi, 2), category

# ä½¿ç”¨å‡½æ•°
bmi, category = calculate_bmi(70, 1.75)
print(f"BMI: {bmi}, ä½“å‹: {category}")
```

### 1.2 é¢å‘å¯¹è±¡ç¼–ç¨‹

#### ç±»å’Œå¯¹è±¡
```python
class Student:
    def __init__(self, name, student_id):
        self.name = name
        self.student_id = student_id
        self.grades = []

    def add_grade(self, grade):
        self.grades.append(grade)

    def get_average(self):
        return sum(self.grades) / len(self.grades)

    def __str__(self):
        return f"å­¦ç”Ÿ: {self.name}, å­¦å·: {self.student_id}"

# åˆ›å»ºå¯¹è±¡
student1 = Student("ç‹äº”", "2021001")
student1.add_grade(90)
student1.add_grade(85)
print(student1)
print(f"å¹³å‡åˆ†: {student1.get_average()}")
```

### 1.3 æ–‡ä»¶æ“ä½œå’Œå¼‚å¸¸å¤„ç†

```python
# æ–‡ä»¶è¯»å†™
def save_student_data(students, filename):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            for student in students:
                f.write(f"{student.name},{student.student_id},{student.get_average()}\n")
        print("æ•°æ®ä¿å­˜æˆåŠŸ")
    except IOError as e:
        print(f"æ–‡ä»¶æ“ä½œé”™è¯¯: {e}")

def load_student_data(filename):
    students = []
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                name, student_id, avg_grade = line.strip().split(',')
                print(f"å­¦ç”Ÿ: {name}, å¹³å‡åˆ†: {avg_grade}")
    except FileNotFoundError:
        print("æ–‡ä»¶ä¸å­˜åœ¨")
    return students
```

## ğŸ“Š ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®åˆ†æåŸºç¡€ (2ä¸ªæœˆ)

### 2.1 NumPyæ•°å€¼è®¡ç®—

#### æ•°ç»„æ“ä½œ
```python
import numpy as np

# åˆ›å»ºæ•°ç»„
arr1 = np.array([1, 2, 3, 4, 5])
arr2 = np.random.randn(3, 3)  # 3x3éšæœºæ•°ç»„
arr3 = np.zeros((5, 5))       # 5x5é›¶æ•°ç»„
arr4 = np.ones((2, 3))        # 2x3å•ä½æ•°ç»„

# æ•°ç»„è¿ç®—
result = arr1 + 10            # æ•°ç»„ä¸æ ‡é‡è¿ç®—
dot_product = np.dot(arr1, arr1)  # ç‚¹ç§¯

# çŸ©é˜µæ“ä½œ
matrix = np.array([[1, 2], [3, 4]])
transpose = matrix.T          # è½¬ç½®
inverse = np.linalg.inv(matrix)  # é€†çŸ©é˜µ
```

#### ç»Ÿè®¡åˆ†æ
```python
# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
data = np.random.normal(100, 15, 1000)  # æ­£æ€åˆ†å¸ƒ

# ç»Ÿè®¡åˆ†æ
mean = np.mean(data)           # å‡å€¼
median = np.median(data)       # ä¸­ä½æ•°
std = np.std(data)            # æ ‡å‡†å·®
percentiles = np.percentile(data, [25, 50, 75])  # å››åˆ†ä½æ•°

print(f"å‡å€¼: {mean:.2f}")
print(f"æ ‡å‡†å·®: {std:.2f}")
print(f"å››åˆ†ä½æ•°: {percentiles}")
```

### 2.2 Pandasæ•°æ®å¤„ç†

#### DataFrameæ“ä½œ
```python
import pandas as pd

# åˆ›å»ºDataFrame
data = {
    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'],
    'å¹´é¾„': [25, 30, 35, 28],
    'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'],
    'è–ªèµ„': [15000, 20000, 18000, 22000]
}
df = pd.DataFrame(data)

# æ•°æ®ç­›é€‰
high_salary = df[df['è–ªèµ„'] > 18000]  # è–ªèµ„å¤§äº18000çš„å‘˜å·¥
beijing_employees = df[df['åŸå¸‚'] == 'åŒ—äº¬']  # åŒ—äº¬å‘˜å·¥

# æ•°æ®èšåˆ
city_avg_salary = df.groupby('åŸå¸‚')['è–ªèµ„'].mean()
age_stats = df['å¹´é¾„'].describe()

print("åŸå¸‚å¹³å‡è–ªèµ„:")
print(city_avg_salary)
```

#### æ•°æ®å¤„ç†
```python
# è¯»å–CSVæ–‡ä»¶
df = pd.read_csv('sales_data.csv')

# æ•°æ®æ¸…æ´—
# å¤„ç†ç¼ºå¤±å€¼
df.dropna(inplace=True)                    # åˆ é™¤ç¼ºå¤±å€¼
df.fillna(0, inplace=True)                 # å¡«å……ç¼ºå¤±å€¼

# å¤„ç†é‡å¤å€¼
df.drop_duplicates(inplace=True)

# æ•°æ®è½¬æ¢
df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])    # è½¬æ¢æ—¥æœŸæ ¼å¼
df['å¹´ä»½'] = df['æ—¥æœŸ'].dt.year            # æå–å¹´ä»½

# æ•°æ®åˆ†æ
monthly_sales = df.groupby('å¹´ä»½')['é”€å”®é¢'].sum()
best_month = monthly_sales.idxmax()
```

### 2.3 Matplotlibæ•°æ®å¯è§†åŒ–

#### åŸºç¡€å›¾è¡¨
```python
import matplotlib.pyplot as plt
import numpy as np

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ç”Ÿæˆæ•°æ®
months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ']
sales = [120, 150, 180, 160, 200, 240]
profits = [30, 45, 60, 50, 80, 95]

# åˆ›å»ºå›¾è¡¨
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# æŠ˜çº¿å›¾
ax1.plot(months, sales, marker='o', linewidth=2, label='é”€å”®é¢')
ax1.plot(months, profits, marker='s', linewidth=2, label='åˆ©æ¶¦')
ax1.set_title('æœˆåº¦é”€å”®è¶‹åŠ¿')
ax1.set_xlabel('æœˆä»½')
ax1.set_ylabel('é‡‘é¢(ä¸‡å…ƒ)')
ax1.legend()
ax1.grid(True)

# æŸ±çŠ¶å›¾
ax2.bar(months, sales, alpha=0.7, color='skyblue')
ax2.set_title('æœˆåº¦é”€å”®é¢')
ax2.set_xlabel('æœˆä»½')
ax2.set_ylabel('é”€å”®é¢(ä¸‡å…ƒ)')

plt.tight_layout()
plt.show()
```

## ğŸ¤– ç¬¬ä¸‰é˜¶æ®µï¼šæœºå™¨å­¦ä¹ å…¥é—¨ (3ä¸ªæœˆ)

### 3.1 ç›‘ç£å­¦ä¹ åŸºç¡€

#### çº¿æ€§å›å½’
```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # ç‰¹å¾
y = 2 * X + 1 + np.random.randn(100, 1) * 2  # ç›®æ ‡å€¼

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"æ¨¡å‹å‚æ•°: æ–œç‡={model.coef_[0][0]:.2f}, æˆªè·={model.intercept_[0]:.2f}")
print(f"å‡æ–¹è¯¯å·®: {mse:.2f}")
print(f"RÂ²åˆ†æ•°: {r2:.2f}")
```

#### é€»è¾‘å›å½’
```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, confusion_matrix

# ç”Ÿæˆåˆ†ç±»æ•°æ®
X, y = make_classification(
    n_samples=1000, n_features=2, n_redundant=0,
    n_informative=2, n_clusters_per_class=1, random_state=42
)

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# åˆ›å»ºæ¨¡å‹
model = LogisticRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"å‡†ç¡®ç‡: {accuracy:.2f}")
print("æ··æ·†çŸ©é˜µ:")
print(conf_matrix)
```

### 3.2 å†³ç­–æ ‘å’Œéšæœºæ£®æ—

#### å†³ç­–æ ‘åˆ†ç±»
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt

# åˆ›å»ºå†³ç­–æ ‘æ¨¡å‹
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model.fit(X_train, y_train)

# é¢„æµ‹
y_pred_dt = dt_model.predict(X_test)
accuracy_dt = accuracy_score(y_test, y_pred_dt)

print(f"å†³ç­–æ ‘å‡†ç¡®ç‡: {accuracy_dt:.2f}")

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(15, 10))
tree.plot_tree(dt_model, feature_names=['ç‰¹å¾1', 'ç‰¹å¾2'],
               class_names=['ç±»åˆ«0', 'ç±»åˆ«1'], filled=True)
plt.show()
```

#### éšæœºæ£®æ—
```python
from sklearn.ensemble import RandomForestClassifier

# åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42
)
rf_model.fit(X_train, y_train)

# ç‰¹å¾é‡è¦æ€§
feature_importance = rf_model.feature_importances_
print(f"ç‰¹å¾é‡è¦æ€§: {feature_importance}")

# é¢„æµ‹
y_pred_rf = rf_model.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"éšæœºæ£®æ—å‡†ç¡®ç‡: {accuracy_rf:.2f}")
```

### 3.3 æ¨¡å‹è¯„ä¼°å’Œè°ƒä¼˜

#### äº¤å‰éªŒè¯
```python
from sklearn.model_selection import cross_val_score, GridSearchCV

# äº¤å‰éªŒè¯
scores = cross_val_score(rf_model, X, y, cv=5)
print(f"äº¤å‰éªŒè¯åˆ†æ•°: {scores}")
print(f"å¹³å‡å‡†ç¡®ç‡: {scores.mean():.2f} (Â±{scores.std():.2f})")

# ç½‘æ ¼æœç´¢è°ƒä¼˜
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy'
)
grid_search.fit(X_train, y_train)

print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.2f}")
```

## ğŸ§  ç¬¬å››é˜¶æ®µï¼šæ·±åº¦å­¦ä¹  (3ä¸ªæœˆ)

### 4.1 ç¥ç»ç½‘ç»œåŸºç¡€

#### ä½¿ç”¨TensorFlow/Keras
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# åˆ›å»ºç®€å•çš„ç¥ç»ç½‘ç»œ
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(2,)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# æ¨¡å‹æ‘˜è¦
model.summary()

# è®­ç»ƒæ¨¡å‹
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# è¯„ä¼°æ¨¡å‹
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.2f}")
```

#### æ·±åº¦å­¦ä¹ é¡¹ç›®ï¼šå›¾åƒåˆ†ç±»
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16

# æ•°æ®å¢å¼º
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

# æ•°æ®åŠ è½½
train_generator = train_datagen.flow_from_directory(
    'data/images',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# å†»ç»“é¢„è®­ç»ƒå±‚
for layer in base_model.layers:
    layer.trainable = False

# æ·»åŠ è‡ªå®šä¹‰å±‚
model = keras.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
```

### 4.2 è‡ªç„¶è¯­è¨€å¤„ç†

#### æ–‡æœ¬åˆ†ç±»
```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# æ–‡æœ¬é¢„å¤„ç†
texts = [
    "è¿™ä¸ªäº§å“å¾ˆå¥½ç”¨",
    "æœåŠ¡æ€åº¦å¾ˆå·®",
    "ç‰©æµé€Ÿåº¦å¾ˆå¿«",
    "è´¨é‡æœ‰é—®é¢˜"
]
labels = [1, 0, 1, 0]  # 1:æ­£é¢, 0:è´Ÿé¢

# åˆ†è¯å’Œç¼–ç 
tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# åºåˆ—å¡«å……
max_length = 10
padded_sequences = pad_sequences(sequences, maxlen=max_length)

# æ„å»ºæ–‡æœ¬åˆ†ç±»æ¨¡å‹
model = keras.Sequential([
    layers.Embedding(1000, 16, input_length=max_length),
    layers.LSTM(32),
    layers.Dense(16, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
```

## ğŸ¯ ç¬¬äº”é˜¶æ®µï¼šä¸“ä¸šæ–¹å‘é€‰æ‹© (4ä¸ªæœˆ)

### 5.1 è®¡ç®—æœºè§†è§‰æ–¹å‘

#### ç›®æ ‡æ£€æµ‹é¡¹ç›®
```python
import cv2
import numpy as np

# ä½¿ç”¨OpenCVè¿›è¡Œç›®æ ‡æ£€æµ‹
def detect_objects(image_path):
    # è¯»å–å›¾åƒ
    image = cv2.imread(image_path)

    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # ä½¿ç”¨Haarçº§è”åˆ†ç±»å™¨æ£€æµ‹äººè„¸
    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)

    # ç»˜åˆ¶æ£€æµ‹æ¡†
    for (x, y, w, h) in faces:
        cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)

    return image

# æ˜¾ç¤ºç»“æœ
result_image = detect_objects('test_image.jpg')
cv2.imshow('Object Detection', result_image)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

### 5.2 è‡ªç„¶è¯­è¨€å¤„ç†æ–¹å‘

#### æƒ…æ„Ÿåˆ†æç³»ç»Ÿ
```python
from transformers import pipeline

# ä½¿ç”¨é¢„è®­ç»ƒçš„æƒ…æ„Ÿåˆ†ææ¨¡å‹
classifier = pipeline("sentiment-analysis")

def analyze_sentiment(text):
    result = classifier(text)[0]
    sentiment = result['label']
    confidence = result['score']

    return {
        'sentiment': sentiment,
        'confidence': confidence,
        'text': text
    }

# æ‰¹é‡åˆ†æ
comments = [
    "è¿™å®¶é¤å…çš„é£Ÿç‰©å¾ˆç¾å‘³",
    "ç­‰å¾…æ—¶é—´å¤ªé•¿äº†",
    "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œç¯å¢ƒä¹Ÿä¸é”™"
]

for comment in comments:
    result = analyze_sentiment(comment)
    print(f"æ–‡æœ¬: {comment}")
    print(f"æƒ…æ„Ÿ: {result['sentiment']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
    print("-" * 50)
```

### 5.3 å¼ºåŒ–å­¦ä¹ æ–¹å‘

#### Q-Learningç®—æ³•å®ç°
```python
import numpy as np

class QLearningAgent:
    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = 1.0  # æ¢ç´¢ç‡
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.q_table = np.zeros((state_size, action_size))

    def choose_action(self, state):
        if np.random.random() <= self.epsilon:
            return np.random.randint(self.action_size)
        else:
            return np.argmax(self.q_table[state])

    def learn(self, state, action, reward, next_state):
        current_q = self.q_table[state, action]
        max_next_q = np.max(self.q_table[next_state])
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        self.q_table[state, action] = new_q

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# è®­ç»ƒæ™ºèƒ½ä½“
agent = QLearningAgent(state_size=16, action_size=4)
episodes = 1000

for episode in range(episodes):
    state = 0  # åˆå§‹çŠ¶æ€
    done = False

    while not done:
        action = agent.choose_action(state)
        reward, next_state, done = environment_step(state, action)
        agent.learn(state, action, reward, next_state)
        state = next_state
```

## ğŸ“ˆ å­¦ä¹ æ—¶é—´è§„åˆ’

### åŸºç¡€é˜¶æ®µ (å‰3ä¸ªæœˆ)
- **ç¬¬1ä¸ªæœˆ**: PythonåŸºç¡€è¯­æ³•å’Œæ•°æ®ç»“æ„
- **ç¬¬2ä¸ªæœˆ**: é¢å‘å¯¹è±¡ç¼–ç¨‹å’Œæ–‡ä»¶æ“ä½œ
- **ç¬¬3ä¸ªæœˆ**: å¸¸ç”¨åº“ä½¿ç”¨å’Œå°é¡¹ç›®å®è·µ

### æ•°æ®åˆ†æé˜¶æ®µ (ç¬¬4-5ä¸ªæœˆ)
- **ç¬¬4ä¸ªæœˆ**: NumPyå’ŒPandasæ·±å…¥å­¦ä¹ 
- **ç¬¬5ä¸ªæœˆ**: æ•°æ®å¯è§†åŒ–å’Œå°å‹æ•°æ®åˆ†æé¡¹ç›®

### æœºå™¨å­¦ä¹ é˜¶æ®µ (ç¬¬6-8ä¸ªæœˆ)
- **ç¬¬6ä¸ªæœˆ**: ç›‘ç£å­¦ä¹ ç®—æ³•å’Œæ¨¡å‹è¯„ä¼°
- **ç¬¬7ä¸ªæœˆ**: é›†æˆå­¦ä¹ å’Œç‰¹å¾å·¥ç¨‹
- **ç¬¬8ä¸ªæœˆ**: å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®

### æ·±åº¦å­¦ä¹ é˜¶æ®µ (ç¬¬9-11ä¸ªæœˆ)
- **ç¬¬9ä¸ªæœˆ**: ç¥ç»ç½‘ç»œåŸºç¡€å’ŒTensorFlow
- **ç¬¬10ä¸ªæœˆ**: CNNå’Œå›¾åƒå¤„ç†
- **ç¬¬11ä¸ªæœˆ**: RNNå’Œè‡ªç„¶è¯­è¨€å¤„ç†

### ä¸“ä¸šæ–¹å‘é˜¶æ®µ (ç¬¬12-15ä¸ªæœˆ)
- **ç¬¬12-15ä¸ªæœˆ**: é€‰æ‹©ä¸“ä¸šæ–¹å‘å¹¶æ·±å…¥å®è·µ

## ğŸ› ï¸ å®è·µé¡¹ç›®å»ºè®®

### åˆçº§é¡¹ç›® (1-3ä¸ªæœˆ)
1. **ä¸ªäººè´¢åŠ¡ç®¡ç†ç³»ç»Ÿ**: ä½¿ç”¨Pandasåˆ†ææ”¶æ”¯æ•°æ®
2. **å¤©æ°”æ•°æ®å¯è§†åŒ–**: æŠ“å–å¤©æ°”æ•°æ®å¹¶åˆ¶ä½œå›¾è¡¨
3. **ç®€å•è®¡ç®—å™¨**: Python GUIåº”ç”¨å¼€å‘

### ä¸­çº§é¡¹ç›® (4-8ä¸ªæœˆ)
1. **æˆ¿ä»·é¢„æµ‹ç³»ç»Ÿ**: çº¿æ€§å›å½’å’Œå¤šé¡¹å¼å›å½’
2. **å®¢æˆ·æµå¤±é¢„æµ‹**: é€»è¾‘å›å½’å’Œå†³ç­–æ ‘
3. **å›¾ä¹¦æ¨èç³»ç»Ÿ**: ååŒè¿‡æ»¤ç®—æ³•

### é«˜çº§é¡¹ç›® (9-12ä¸ªæœˆ)
1. **å›¾åƒåˆ†ç±»åº”ç”¨**: æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²
2. **èŠå¤©æœºå™¨äºº**: NLPå’Œå¯¹è¯ç³»ç»Ÿ
3. **è‚¡ç¥¨ä»·æ ¼é¢„æµ‹**: æ—¶é—´åºåˆ—åˆ†æ

## ğŸ’¡ å­¦ä¹ å»ºè®®

### 1. ç†è®ºä¸å®è·µç»“åˆ
- æ¯å­¦å®Œä¸€ä¸ªæ¦‚å¿µï¼Œç«‹å³ç¼–å†™ä»£ç å®è·µ
- å‚ä¸Kaggleç«èµ›æå‡å®æˆ˜èƒ½åŠ›
- é˜…è¯»ç»å…¸è®ºæ–‡äº†è§£ç®—æ³•åŸç†

### 2. å»ºç«‹é¡¹ç›®ä½œå“é›†
- æ¯ä¸ªæœˆå®Œæˆä¸€ä¸ªå®Œæ•´é¡¹ç›®
- å°†é¡¹ç›®éƒ¨ç½²åˆ°GitHubæˆ–ä¸ªäººç½‘ç«™
- ç¼–å†™è¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£

### 3. æŒç»­å­¦ä¹ 
- å…³æ³¨AIé¢†åŸŸæœ€æ–°å‘å±•
- å‚åŠ æŠ€æœ¯meetupå’Œä¼šè®®
- åŠ å…¥AIå­¦ä¹ ç¤¾åŒº

### 4. æ•°å­¦åŸºç¡€
- çº¿æ€§ä»£æ•°: çŸ©é˜µè¿ç®—ã€ç‰¹å¾å€¼
- æ¦‚ç‡ç»Ÿè®¡: æ¦‚ç‡åˆ†å¸ƒã€å‡è®¾æ£€éªŒ
- å¾®ç§¯åˆ†: å¯¼æ•°ã€æ¢¯åº¦ä¸‹é™

## ğŸ“š æ¨èèµ„æº

### åœ¨çº¿è¯¾ç¨‹
- **Coursera**: Andrew Ngçš„æœºå™¨å­¦ä¹ è¯¾ç¨‹
- **Udacity**: AIçº³ç±³å­¦ä½
- **edX**: MITçš„è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹

### ä¹¦ç±æ¨è
- ã€ŠPythonç¼–ç¨‹ï¼šä»å…¥é—¨åˆ°å®è·µã€‹
- ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹
- ã€Šæ·±åº¦å­¦ä¹ ã€‹- Ian Goodfellow

### å¼€æºé¡¹ç›®
- **Scikit-learn**: æœºå™¨å­¦ä¹ åº“
- **TensorFlow**: æ·±åº¦å­¦ä¹ æ¡†æ¶
- **Fast.ai**: æ·±åº¦å­¦ä¹ æœ€ä½³å®è·µ

---

**AIå­¦ä¹ æ˜¯ä¸€ä¸ªé•¿æœŸçš„è¿‡ç¨‹ï¼Œéœ€è¦æŒç»­å­¦ä¹ å’Œå®è·µã€‚å¸Œæœ›è¿™ä¸ªå­¦ä¹ è·¯å¾„èƒ½å¸®åŠ©ä½ ä»é›¶åŸºç¡€æˆé•¿ä¸ºAIå·¥ç¨‹å¸ˆï¼**

*å¦‚æœåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œæ¬¢è¿åœ¨è¯„è®ºåŒºäº¤æµè®¨è®ºã€‚*